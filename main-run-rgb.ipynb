{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"main-run-rgb.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPbBirNqGQEspGSwm5QQQBn"},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3b580ad6a9a140b596452def519a70d0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3c8af3c7a8d044e7a1a56ed3bf67236b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3372b39ec96a42d394305e40a2ee9fb5","IPY_MODEL_895a7ce4bb6c40669269281cba8fac17"]}},"3c8af3c7a8d044e7a1a56ed3bf67236b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3372b39ec96a42d394305e40a2ee9fb5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_373de57c36944b93bf490c46b959bd39","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":87306240,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":87306240,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9ad2a4241d104f8b9ec333892749d861"}},"895a7ce4bb6c40669269281cba8fac17":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_03302cef012844028e7883096d44b45b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 83.3M/83.3M [1:31:13&lt;00:00, 15.9kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fa83010fb8e044fd9a600c31622f3f5f"}},"373de57c36944b93bf490c46b959bd39":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9ad2a4241d104f8b9ec333892749d861":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"03302cef012844028e7883096d44b45b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"fa83010fb8e044fd9a600c31622f3f5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-07NoMvInQMN","executionInfo":{"status":"ok","timestamp":1611390261235,"user_tz":-60,"elapsed":76824,"user":{"displayName":"Luca Neromoro","photoUrl":"","userId":"08197997045770955362"}},"outputId":"9b842b1c-6f3f-48b9-accd-39db1622ab84"},"source":["!pip install import_ipynb\r\n","import import_ipynb\r\n","from google.colab import drive\r\n","drive.mount('/content/gdrive',  force_remount=True)\r\n","%cd /content/gdrive/My Drive/Project FPAR2/ego-rnn\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting import_ipynb\n","  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n","Building wheels for collected packages: import-ipynb\n","  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp36-none-any.whl size=2976 sha256=68adcba606cab51717e592fe6f1ef0e37ad030ce653ed21940bd734f077589ae\n","  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n","Successfully built import-ipynb\n","Installing collected packages: import-ipynb\n","Successfully installed import-ipynb-0.1.3\n","Mounted at /content/gdrive\n","/content/gdrive/My Drive/Project FPAR2/ego-rnn\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["3b580ad6a9a140b596452def519a70d0","3c8af3c7a8d044e7a1a56ed3bf67236b","3372b39ec96a42d394305e40a2ee9fb5","895a7ce4bb6c40669269281cba8fac17","373de57c36944b93bf490c46b959bd39","9ad2a4241d104f8b9ec333892749d861","03302cef012844028e7883096d44b45b","fa83010fb8e044fd9a600c31622f3f5f"]},"id":"ErH4I0eMWqtj","executionInfo":{"status":"error","timestamp":1611396222724,"user_tz":-60,"elapsed":78667,"user":{"displayName":"Luca Neromoro","photoUrl":"","userId":"08197997045770955362"}},"outputId":"687e7d19-01d7-4789-c4a4-3eca3e8ca625"},"source":["\n","from __future__ import print_function, division\n","from objectAttentionModelConvAugmentedLSTM import *\n","#from objectAttentionModelConvLSTMVisualize import *\n","from spatial_transforms import (Compose, ToTensor, CenterCrop, Scale, Normalize, MultiScaleCornerCrop,\n","                                RandomHorizontalFlip)\n","!pip install tensorboardX\n","from tensorboardX import SummaryWriter\n","#from MyConvGruCell import *\n","from makeDatasetRGBSelection import *\n","\n","import argparse\n","import sys\n","from resnetMod import *\n","from plotting import *\n","\n","def main_run(dataset, stage, train_data_dir, val_data_dir, stage1_dict, out_dir, seqLen, trainBatchSize,\n","             valBatchSize, numEpochs, lr1, decay_factor, decay_step, memSize, cam):\n","\n","    if dataset == 'gtea61':\n","        num_classes = 61\n","    elif dataset == 'gtea71':\n","      num_classes = 71\n","    elif dataset == 'gtea_gaze':\n","        num_classes = 44\n","    elif dataset == 'egtea':\n","        num_classes = 106\n","    else:\n","        print('Dataset not found')\n","        sys.exit()\n","\n","    model_folder = out_dir  # Dir for saving models and log files\n","    \n","\n","    # Log files\n","    writer = SummaryWriter(model_folder)\n","    train_log_loss = open((model_folder + '/train_log_loss.txt'), 'w')\n","    train_log_acc = open((model_folder + '/train_log_acc.txt'), 'w')\n","    val_log_loss = open((model_folder + '/val_log_loss.txt'), 'w')\n","    val_log_acc = open((model_folder + '/val_log_acc.txt'), 'w')\n","\n","\n","    # Data loader\n","    normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    spatial_transform = Compose([Scale(256), RandomHorizontalFlip(), MultiScaleCornerCrop([1, 0.875, 0.75, 0.65625], 224), ToTensor(), normalize])\n","    from torchvision.transforms import Resize\n","    spatial_transform_mmap = Compose([Resize(256),ToTensor()])\n","    vid_seq_train = makeDataset(train_data_dir,\n","                                spatial_transform=spatial_transform, seqLen=seqLen, fmt='.png')\n","\n","    train_loader = torch.utils.data.DataLoader(vid_seq_train, batch_size=trainBatchSize,\n","                            shuffle=True, num_workers=4, pin_memory=True)\n","    if val_data_dir is not None:\n","\n","        vid_seq_val = makeDataset(val_data_dir,\n","                                   spatial_transform=Compose([Scale(256), CenterCrop(224), ToTensor(), normalize]),\n","                                   seqLen=seqLen, train=False, fmt='.png')\n","\n","        val_loader = torch.utils.data.DataLoader(vid_seq_val, batch_size=valBatchSize,\n","                                shuffle=False, num_workers=2, pin_memory=True)\n","        valInstances = vid_seq_val.__len__()\n","\n","\n","    trainInstances = vid_seq_train.__len__()\n","\n","    train_params = []\n","    if stage == 1:\n","\n","        model = attentionModel(num_classes=num_classes, mem_size=memSize)\n","        model.train(False)\n","        for params in model.parameters():\n","            params.requires_grad = False\n","    else:\n","\n","        model = attentionModel(num_classes=num_classes, mem_size=memSize)\n","        model.load_state_dict(torch.load(stage1_dict))\n","        model.train(False)\n","        for params in model.parameters():\n","            params.requires_grad = False\n","        #\n","        for params in model.resNet.layer4[0].conv1.parameters():\n","            params.requires_grad = True\n","            train_params += [params]\n","\n","        for params in model.resNet.layer4[0].conv2.parameters():\n","            params.requires_grad = True\n","            train_params += [params]\n","\n","        for params in model.resNet.layer4[1].conv1.parameters():\n","            params.requires_grad = True\n","            train_params += [params]\n","\n","        for params in model.resNet.layer4[1].conv2.parameters():\n","            params.requires_grad = True\n","            train_params += [params]\n","\n","        for params in model.resNet.layer4[2].conv1.parameters():\n","            params.requires_grad = True\n","            train_params += [params]\n","        #\n","        for params in model.resNet.layer4[2].conv2.parameters():\n","            params.requires_grad = True\n","            train_params += [params]\n","        #\n","        for params in model.resNet.fc.parameters():\n","            params.requires_grad = True\n","            train_params += [params]\n","\n","        model.resNet.layer4[0].conv1.train(True)\n","        model.resNet.layer4[0].conv2.train(True)\n","        model.resNet.layer4[1].conv1.train(True)\n","        model.resNet.layer4[1].conv2.train(True)\n","        model.resNet.layer4[2].conv1.train(True)\n","        model.resNet.layer4[2].conv2.train(True)\n","        model.resNet.fc.train(True)\n","\n","    for params in model.lstm_cell.parameters():\n","        params.requires_grad = True\n","        train_params += [params]\n","\n","    for params in model.classifier.parameters():\n","        params.requires_grad = True\n","        train_params += [params]\n","\n","    model.lstm_cell.train(True)\n","    model.classifier.train(True)\n","    model.cuda()\n","\n","    loss_fn = nn.CrossEntropyLoss()\n","\n","    optimizer_fn = torch.optim.Adam(train_params, lr=lr1, weight_decay=4e-5, eps=1e-4)\n","\n","    optim_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=decay_step,\n","                                                           gamma=decay_factor)\n","    \n","    train_loss_list=[]\n","    val_loss_list=[]\n","    train_acc_list=[]\n","    val_acc_list=[]\n","\n","    train_iter = 0\n","    min_accuracy = 0\n","    print('train')\n","\n","    for epoch in range(numEpochs):\n","        optim_scheduler.step()\n","        epoch_loss = 0\n","        numCorrTrain = 0\n","        trainSamples = 0\n","        iterPerEpoch = 0\n","        model.lstm_cell.train(True)\n","        model.classifier.train(True)\n","        writer.add_scalar('lr', optimizer_fn.param_groups[0]['lr'], epoch+1)\n","        if stage == 2:\n","            model.resNet.layer4[0].conv1.train(True)\n","            model.resNet.layer4[0].conv2.train(True)\n","            model.resNet.layer4[1].conv1.train(True)\n","            model.resNet.layer4[1].conv2.train(True)\n","            model.resNet.layer4[2].conv1.train(True)\n","            model.resNet.layer4[2].conv2.train(True)\n","            model.resNet.fc.train(True)\n","            \n","        for i, (inputs, targets) in enumerate(train_loader):\n","            train_iter += 1\n","            iterPerEpoch += 1\n","            optimizer_fn.zero_grad()\n","            inputVariable = Variable(inputs.permute(1, 0, 2, 3, 4).cuda())\n","            labelVariable = Variable(targets.cuda())\n","            trainSamples += inputs.size(0)\n","            output_label, _ = model(inputVariable)\n","            #output_label, _ = model(inputVariable)\n","            loss = loss_fn(output_label, labelVariable)\n","            loss.backward()\n","            optimizer_fn.step()\n","            _, predicted = torch.max(output_label.data, 1)\n","            numCorrTrain += (predicted == targets.cuda()).sum().item()\n","            epoch_loss += loss.data\n","        avg_loss = epoch_loss/iterPerEpoch\n","        trainAccuracy = (numCorrTrain / trainSamples) * 100\n","\n","        train_loss_list.append(avg_loss)\n","        train_acc_list.append(trainAccuracy)\n","\n","        print('Train: Epoch = {} | Loss = {} | Accuracy = {}'.format(epoch+1, avg_loss, trainAccuracy))\n","        writer.add_scalar('train/epoch_loss', avg_loss, epoch+1)\n","        writer.add_scalar('train/accuracy', trainAccuracy, epoch+1)\n","        if val_data_dir is not None:\n","            if (epoch+1) % 1 == 0:\n","                model.train(False)\n","                val_loss_epoch = 0\n","                val_iter = 0\n","                val_samples = 0\n","                numCorr = 0\n","                for j, (inputs, targets) in enumerate(val_loader):\n","                    val_iter += 1\n","                    val_samples += inputs.size(0)\n","                    inputVariable = Variable(inputs.permute(1, 0, 2, 3, 4).cuda(), volatile=True)\n","                    labelVariable = Variable(targets.cuda(async=True), volatile=True)\n","                    output_label, _ = model(inputVariable)\n","                    val_loss = loss_fn(output_label, labelVariable)\n","                    val_loss_epoch += val_loss.data\n","                    _, predicted = torch.max(output_label.data, 1)\n","                    numCorr += (predicted == targets.cuda()).sum().item()\n","                val_accuracy = (numCorr / val_samples) * 100\n","                avg_val_loss = val_loss_epoch / val_iter\n","\n","                val_loss_list.append(avg_val_loss)\n","                val_acc_list.append(val_accuracy)\n","                print('Val: Epoch = {} | Loss {} | Accuracy = {}'.format(epoch + 1, avg_val_loss, val_accuracy))\n","                writer.add_scalar('val/epoch_loss', avg_val_loss, epoch + 1)\n","                writer.add_scalar('val/accuracy', val_accuracy, epoch + 1)\n","              \n","                if val_accuracy > min_accuracy:\n","                    save_path_model = (model_folder + '/model_rgb_state_dict.pth')\n","                    torch.save(model.state_dict(), save_path_model)\n","                    min_accuracy = val_accuracy\n","            else:\n","                if (epoch+1) % 10 == 0:\n","                  i=0\n","                  save_path_model = (model_folder + '/model_rgb_state_dict_epoch' + str(epoch+1) + '.pth')\n","                  torch.save(model.state_dict(), save_path_model)\n","    '''\n","    train_log_loss.close()\n","    train_log_acc.close()\n","    val_log_acc.close()\n","    val_log_loss.close()\n","    writer.export_scalars_to_json(model_folder + \"/all_scalars.json\")\n","    writer.close()\n","    '''\n","    plotting(train_loss_list, val_loss_list,train_acc_list, val_acc_list)\n","\n","\n","def __main__():\n","\n","    main_run(dataset='gtea61', stage=1, train_data_dir='./GTEA61/processed_frames2', val_data_dir='./GTEA61/processed_frames2', stage1_dict='./experiments/selection+augm/stage1/model_rgb_state_dict.pth', out_dir='experiments/selection+augm/stage2', seqLen=16, trainBatchSize=32,\n","             valBatchSize=64, numEpochs=150, lr1=1e-4, decay_factor=0.1, decay_step=[25,75,150], memSize=512, cam=True)\n","\n","__main__()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["importing Jupyter notebook from objectAttentionModelConvAugmentedLSTM.ipynb\n","importing Jupyter notebook from resnetMod.ipynb\n","importing Jupyter notebook from MyConvAugmentedLSTMCell.ipynb\n","importing Jupyter notebook from spatial_transforms.ipynb\n","Collecting tensorboardX\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 317kB 5.7MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.19.5)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (51.3.3)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.1\n","importing Jupyter notebook from makeDatasetRGBSelection.ipynb\n","importing Jupyter notebook from plotting.ipynb\n"],"name":"stdout"},{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3b580ad6a9a140b596452def519a70d0","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=87306240.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["MyConvAugmentedLSTMCell.ipynb:50: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvAugmentedLSTMCell.ipynb:51: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","MyConvAugmentedLSTMCell.ipynb:52: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvAugmentedLSTMCell.ipynb:54: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvAugmentedLSTMCell.ipynb:55: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","MyConvAugmentedLSTMCell.ipynb:56: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvAugmentedLSTMCell.ipynb:58: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvAugmentedLSTMCell.ipynb:59: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","MyConvAugmentedLSTMCell.ipynb:60: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvAugmentedLSTMCell.ipynb:62: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvAugmentedLSTMCell.ipynb:63: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","MyConvAugmentedLSTMCell.ipynb:64: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvAugmentedLSTMCell.ipynb:67: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvAugmentedLSTMCell.ipynb:68: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","MyConvAugmentedLSTMCell.ipynb:69: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvAugmentedLSTMCell.ipynb:71: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvAugmentedLSTMCell.ipynb:72: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","MyConvAugmentedLSTMCell.ipynb:73: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvAugmentedLSTMCell.ipynb:75: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvAugmentedLSTMCell.ipynb:76: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","MyConvAugmentedLSTMCell.ipynb:77: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvAugmentedLSTMCell.ipynb:79: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvAugmentedLSTMCell.ipynb:80: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","MyConvAugmentedLSTMCell.ipynb:81: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n"],"name":"stderr"},{"output_type":"stream","text":["train\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["Train: Epoch = 1 | Loss = 4.108048915863037 | Accuracy = 4.424778761061947\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-b9ffc4adee2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    236\u001b[0m              valBatchSize=64, numEpochs=150, lr1=1e-4, decay_factor=0.1, decay_step=[25,75,150], memSize=512, cam=True)\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m \u001b[0m__main__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-b9ffc4adee2b>\u001b[0m in \u001b[0;36m__main__\u001b[0;34m()\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     main_run(dataset='gtea61', stage=1, train_data_dir='./GTEA61/processed_frames2', val_data_dir='./GTEA61/processed_frames2', stage1_dict='./experiments/selection+augm/stage1/model_rgb_state_dict.pth', out_dir='experiments/selection+augm/stage2', seqLen=16, trainBatchSize=32,\n\u001b[0;32m--> 236\u001b[0;31m              valBatchSize=64, numEpochs=150, lr1=1e-4, decay_factor=0.1, decay_step=[25,75,150], memSize=512, cam=True)\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0m__main__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-b9ffc4adee2b>\u001b[0m in \u001b[0;36mmain_run\u001b[0;34m(dataset, stage, train_data_dir, val_data_dir, stage1_dict, out_dir, seqLen, trainBatchSize, valBatchSize, numEpochs, lr1, decay_factor, decay_step, memSize, cam)\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0mval_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mnumCorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m                     \u001b[0mval_iter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                     \u001b[0mval_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"70SsyDYj2n6d","executionInfo":{"status":"ok","timestamp":1606683220029,"user_tz":-60,"elapsed":11078,"user":{"displayName":"Luca Neromoro","photoUrl":"","userId":"08197997045770955362"}},"outputId":"4fc4da27-5051-4e5b-885d-732cedef42e0"},"source":["!pip install import_ipynb\n","import import_ipynb\n","from google.colab import drive\n","drive.mount('/content/gdrive',  force_remount=True)\n","%cd /content/gdrive/My Drive/Project FPAR2/ego-rnn\n","\n","num_classes=61\n","memSize=512\n","\n","from countParameters import *\n","from objectAttentionModelConvAugmentedLSTM import *\n","model = attentionModel(num_classes=num_classes, mem_size=memSize)\n","from objectAttentionModelLSTA import *\n","\n","model2 = attentionModel(num_classes=num_classes, mem_size=memSize)\n","\n","n1=count_parameters(model)\n","n2=count_parameters(model2)\n","print(n1)\n","print(n2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: import_ipynb in /usr/local/lib/python3.6/dist-packages (0.1.3)\n","Mounted at /content/gdrive\n","/content/gdrive/My Drive/Project FPAR2/ego-rnn\n","importing Jupyter notebook from countParameters.ipynb\n","importing Jupyter notebook from objectAttentionModelConvAugmentedLSTM.ipynb\n","importing Jupyter notebook from resnetMod.ipynb\n","importing Jupyter notebook from MyConvAugmentedLSTMCell.ipynb\n"],"name":"stdout"},{"output_type":"stream","text":["MyConvAugmentedLSTMCell.ipynb:50: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvAugmentedLSTMCell.ipynb:51: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","MyConvAugmentedLSTMCell.ipynb:52: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvAugmentedLSTMCell.ipynb:54: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvAugmentedLSTMCell.ipynb:55: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","MyConvAugmentedLSTMCell.ipynb:56: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvAugmentedLSTMCell.ipynb:58: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvAugmentedLSTMCell.ipynb:59: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","MyConvAugmentedLSTMCell.ipynb:60: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvAugmentedLSTMCell.ipynb:62: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvAugmentedLSTMCell.ipynb:63: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","MyConvAugmentedLSTMCell.ipynb:64: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n"],"name":"stderr"},{"output_type":"stream","text":["importing Jupyter notebook from objectAttentionModelLSTA.ipynb\n","importing Jupyter notebook from MyConvLSTACell.ipynb\n","importing Jupyter notebook from resNetNew.ipynb\n"],"name":"stdout"},{"output_type":"stream","text":["MyConvLSTACell.ipynb:49: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvLSTACell.ipynb:50: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","MyConvLSTACell.ipynb:51: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvLSTACell.ipynb:53: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvLSTACell.ipynb:54: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","MyConvLSTACell.ipynb:55: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvLSTACell.ipynb:57: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvLSTACell.ipynb:58: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","MyConvLSTACell.ipynb:59: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvLSTACell.ipynb:61: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvLSTACell.ipynb:62: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","MyConvLSTACell.ipynb:63: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvLSTACell.ipynb:100: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvLSTACell.ipynb:101: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","MyConvLSTACell.ipynb:102: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvLSTACell.ipynb:104: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n"],"name":"stderr"},{"output_type":"stream","text":["+-------------------------------------+------------+\n","|               Modules               | Parameters |\n","+-------------------------------------+------------+\n","|            weight_softmax           |   512000   |\n","|         resNet.conv1.weight         |    9408    |\n","|          resNet.bn1.weight          |     64     |\n","|           resNet.bn1.bias           |     64     |\n","|     resNet.layer1.0.conv1.weight    |   36864    |\n","|      resNet.layer1.0.bn1.weight     |     64     |\n","|       resNet.layer1.0.bn1.bias      |     64     |\n","|     resNet.layer1.0.conv2.weight    |   36864    |\n","|      resNet.layer1.0.bn2.weight     |     64     |\n","|       resNet.layer1.0.bn2.bias      |     64     |\n","|     resNet.layer1.1.conv1.weight    |   36864    |\n","|      resNet.layer1.1.bn1.weight     |     64     |\n","|       resNet.layer1.1.bn1.bias      |     64     |\n","|     resNet.layer1.1.conv2.weight    |   36864    |\n","|      resNet.layer1.1.bn2.weight     |     64     |\n","|       resNet.layer1.1.bn2.bias      |     64     |\n","|     resNet.layer1.2.conv1.weight    |   36864    |\n","|      resNet.layer1.2.bn1.weight     |     64     |\n","|       resNet.layer1.2.bn1.bias      |     64     |\n","|     resNet.layer1.2.conv2.weight    |   36864    |\n","|      resNet.layer1.2.bn2.weight     |     64     |\n","|       resNet.layer1.2.bn2.bias      |     64     |\n","|     resNet.layer2.0.conv1.weight    |   73728    |\n","|      resNet.layer2.0.bn1.weight     |    128     |\n","|       resNet.layer2.0.bn1.bias      |    128     |\n","|     resNet.layer2.0.conv2.weight    |   147456   |\n","|      resNet.layer2.0.bn2.weight     |    128     |\n","|       resNet.layer2.0.bn2.bias      |    128     |\n","| resNet.layer2.0.downsample.0.weight |    8192    |\n","| resNet.layer2.0.downsample.1.weight |    128     |\n","|  resNet.layer2.0.downsample.1.bias  |    128     |\n","|     resNet.layer2.1.conv1.weight    |   147456   |\n","|      resNet.layer2.1.bn1.weight     |    128     |\n","|       resNet.layer2.1.bn1.bias      |    128     |\n","|     resNet.layer2.1.conv2.weight    |   147456   |\n","|      resNet.layer2.1.bn2.weight     |    128     |\n","|       resNet.layer2.1.bn2.bias      |    128     |\n","|     resNet.layer2.2.conv1.weight    |   147456   |\n","|      resNet.layer2.2.bn1.weight     |    128     |\n","|       resNet.layer2.2.bn1.bias      |    128     |\n","|     resNet.layer2.2.conv2.weight    |   147456   |\n","|      resNet.layer2.2.bn2.weight     |    128     |\n","|       resNet.layer2.2.bn2.bias      |    128     |\n","|     resNet.layer2.3.conv1.weight    |   147456   |\n","|      resNet.layer2.3.bn1.weight     |    128     |\n","|       resNet.layer2.3.bn1.bias      |    128     |\n","|     resNet.layer2.3.conv2.weight    |   147456   |\n","|      resNet.layer2.3.bn2.weight     |    128     |\n","|       resNet.layer2.3.bn2.bias      |    128     |\n","|     resNet.layer3.0.conv1.weight    |   294912   |\n","|      resNet.layer3.0.bn1.weight     |    256     |\n","|       resNet.layer3.0.bn1.bias      |    256     |\n","|     resNet.layer3.0.conv2.weight    |   589824   |\n","|      resNet.layer3.0.bn2.weight     |    256     |\n","|       resNet.layer3.0.bn2.bias      |    256     |\n","| resNet.layer3.0.downsample.0.weight |   32768    |\n","| resNet.layer3.0.downsample.1.weight |    256     |\n","|  resNet.layer3.0.downsample.1.bias  |    256     |\n","|     resNet.layer3.1.conv1.weight    |   589824   |\n","|      resNet.layer3.1.bn1.weight     |    256     |\n","|       resNet.layer3.1.bn1.bias      |    256     |\n","|     resNet.layer3.1.conv2.weight    |   589824   |\n","|      resNet.layer3.1.bn2.weight     |    256     |\n","|       resNet.layer3.1.bn2.bias      |    256     |\n","|     resNet.layer3.2.conv1.weight    |   589824   |\n","|      resNet.layer3.2.bn1.weight     |    256     |\n","|       resNet.layer3.2.bn1.bias      |    256     |\n","|     resNet.layer3.2.conv2.weight    |   589824   |\n","|      resNet.layer3.2.bn2.weight     |    256     |\n","|       resNet.layer3.2.bn2.bias      |    256     |\n","|     resNet.layer3.3.conv1.weight    |   589824   |\n","|      resNet.layer3.3.bn1.weight     |    256     |\n","|       resNet.layer3.3.bn1.bias      |    256     |\n","|     resNet.layer3.3.conv2.weight    |   589824   |\n","|      resNet.layer3.3.bn2.weight     |    256     |\n","|       resNet.layer3.3.bn2.bias      |    256     |\n","|     resNet.layer3.4.conv1.weight    |   589824   |\n","|      resNet.layer3.4.bn1.weight     |    256     |\n","|       resNet.layer3.4.bn1.bias      |    256     |\n","|     resNet.layer3.4.conv2.weight    |   589824   |\n","|      resNet.layer3.4.bn2.weight     |    256     |\n","|       resNet.layer3.4.bn2.bias      |    256     |\n","|     resNet.layer3.5.conv1.weight    |   589824   |\n","|      resNet.layer3.5.bn1.weight     |    256     |\n","|       resNet.layer3.5.bn1.bias      |    256     |\n","|     resNet.layer3.5.conv2.weight    |   589824   |\n","|      resNet.layer3.5.bn2.weight     |    256     |\n","|       resNet.layer3.5.bn2.bias      |    256     |\n","|     resNet.layer4.0.conv1.weight    |  1179648   |\n","|      resNet.layer4.0.bn1.weight     |    512     |\n","|       resNet.layer4.0.bn1.bias      |    512     |\n","|     resNet.layer4.0.conv2.weight    |  2359296   |\n","|      resNet.layer4.0.bn2.weight     |    512     |\n","|       resNet.layer4.0.bn2.bias      |    512     |\n","| resNet.layer4.0.downsample.0.weight |   131072   |\n","| resNet.layer4.0.downsample.1.weight |    512     |\n","|  resNet.layer4.0.downsample.1.bias  |    512     |\n","|     resNet.layer4.1.conv1.weight    |  2359296   |\n","|      resNet.layer4.1.bn1.weight     |    512     |\n","|       resNet.layer4.1.bn1.bias      |    512     |\n","|     resNet.layer4.1.conv2.weight    |  2359296   |\n","|      resNet.layer4.1.bn2.weight     |    512     |\n","|       resNet.layer4.1.bn2.bias      |    512     |\n","|     resNet.layer4.2.conv1.weight    |  2359296   |\n","|      resNet.layer4.2.bn1.weight     |    512     |\n","|       resNet.layer4.2.bn1.bias      |    512     |\n","|     resNet.layer4.2.conv2.weight    |  2359296   |\n","|      resNet.layer4.2.bn2.weight     |    512     |\n","|       resNet.layer4.2.bn2.bias      |    512     |\n","|            resNet.fc.bias           |    1000    |\n","|      lstm_cell.conv_i_xx.weight     |  2359296   |\n","|       lstm_cell.conv_i_xx.bias      |    512     |\n","|      lstm_cell.conv_i_hh.weight     |  2359296   |\n","|      lstm_cell.conv_f_xx.weight     |  2359296   |\n","|       lstm_cell.conv_f_xx.bias      |    512     |\n","|      lstm_cell.conv_f_hh.weight     |  2359296   |\n","|      lstm_cell.conv_c_xx.weight     |  2359296   |\n","|       lstm_cell.conv_c_xx.bias      |    512     |\n","|      lstm_cell.conv_c_hh.weight     |  2359296   |\n","|      lstm_cell.conv_o_xx.weight     |  2359296   |\n","|       lstm_cell.conv_o_xx.bias      |    512     |\n","|      lstm_cell.conv_o_hh.weight     |  2359296   |\n","|              fc.weight              |   62464    |\n","|               fc.bias               |     61     |\n","+-------------------------------------+------------+\n","Total Trainable Params: 40736613\n","+-------------------------------------+------------+\n","|               Modules               | Parameters |\n","+-------------------------------------+------------+\n","|         resNet.conv1.weight         |    9408    |\n","|          resNet.bn1.weight          |     64     |\n","|           resNet.bn1.bias           |     64     |\n","|     resNet.layer1.0.conv1.weight    |   36864    |\n","|      resNet.layer1.0.bn1.weight     |     64     |\n","|       resNet.layer1.0.bn1.bias      |     64     |\n","|     resNet.layer1.0.conv2.weight    |   36864    |\n","|      resNet.layer1.0.bn2.weight     |     64     |\n","|       resNet.layer1.0.bn2.bias      |     64     |\n","|     resNet.layer1.1.conv1.weight    |   36864    |\n","|      resNet.layer1.1.bn1.weight     |     64     |\n","|       resNet.layer1.1.bn1.bias      |     64     |\n","|     resNet.layer1.1.conv2.weight    |   36864    |\n","|      resNet.layer1.1.bn2.weight     |     64     |\n","|       resNet.layer1.1.bn2.bias      |     64     |\n","|     resNet.layer1.2.conv1.weight    |   36864    |\n","|      resNet.layer1.2.bn1.weight     |     64     |\n","|       resNet.layer1.2.bn1.bias      |     64     |\n","|     resNet.layer1.2.conv2.weight    |   36864    |\n","|      resNet.layer1.2.bn2.weight     |     64     |\n","|       resNet.layer1.2.bn2.bias      |     64     |\n","|     resNet.layer2.0.conv1.weight    |   73728    |\n","|      resNet.layer2.0.bn1.weight     |    128     |\n","|       resNet.layer2.0.bn1.bias      |    128     |\n","|     resNet.layer2.0.conv2.weight    |   147456   |\n","|      resNet.layer2.0.bn2.weight     |    128     |\n","|       resNet.layer2.0.bn2.bias      |    128     |\n","| resNet.layer2.0.downsample.0.weight |    8192    |\n","| resNet.layer2.0.downsample.1.weight |    128     |\n","|  resNet.layer2.0.downsample.1.bias  |    128     |\n","|     resNet.layer2.1.conv1.weight    |   147456   |\n","|      resNet.layer2.1.bn1.weight     |    128     |\n","|       resNet.layer2.1.bn1.bias      |    128     |\n","|     resNet.layer2.1.conv2.weight    |   147456   |\n","|      resNet.layer2.1.bn2.weight     |    128     |\n","|       resNet.layer2.1.bn2.bias      |    128     |\n","|     resNet.layer2.2.conv1.weight    |   147456   |\n","|      resNet.layer2.2.bn1.weight     |    128     |\n","|       resNet.layer2.2.bn1.bias      |    128     |\n","|     resNet.layer2.2.conv2.weight    |   147456   |\n","|      resNet.layer2.2.bn2.weight     |    128     |\n","|       resNet.layer2.2.bn2.bias      |    128     |\n","|     resNet.layer2.3.conv1.weight    |   147456   |\n","|      resNet.layer2.3.bn1.weight     |    128     |\n","|       resNet.layer2.3.bn1.bias      |    128     |\n","|     resNet.layer2.3.conv2.weight    |   147456   |\n","|      resNet.layer2.3.bn2.weight     |    128     |\n","|       resNet.layer2.3.bn2.bias      |    128     |\n","|     resNet.layer3.0.conv1.weight    |   294912   |\n","|      resNet.layer3.0.bn1.weight     |    256     |\n","|       resNet.layer3.0.bn1.bias      |    256     |\n","|     resNet.layer3.0.conv2.weight    |   589824   |\n","|      resNet.layer3.0.bn2.weight     |    256     |\n","|       resNet.layer3.0.bn2.bias      |    256     |\n","| resNet.layer3.0.downsample.0.weight |   32768    |\n","| resNet.layer3.0.downsample.1.weight |    256     |\n","|  resNet.layer3.0.downsample.1.bias  |    256     |\n","|     resNet.layer3.1.conv1.weight    |   589824   |\n","|      resNet.layer3.1.bn1.weight     |    256     |\n","|       resNet.layer3.1.bn1.bias      |    256     |\n","|     resNet.layer3.1.conv2.weight    |   589824   |\n","|      resNet.layer3.1.bn2.weight     |    256     |\n","|       resNet.layer3.1.bn2.bias      |    256     |\n","|     resNet.layer3.2.conv1.weight    |   589824   |\n","|      resNet.layer3.2.bn1.weight     |    256     |\n","|       resNet.layer3.2.bn1.bias      |    256     |\n","|     resNet.layer3.2.conv2.weight    |   589824   |\n","|      resNet.layer3.2.bn2.weight     |    256     |\n","|       resNet.layer3.2.bn2.bias      |    256     |\n","|     resNet.layer3.3.conv1.weight    |   589824   |\n","|      resNet.layer3.3.bn1.weight     |    256     |\n","|       resNet.layer3.3.bn1.bias      |    256     |\n","|     resNet.layer3.3.conv2.weight    |   589824   |\n","|      resNet.layer3.3.bn2.weight     |    256     |\n","|       resNet.layer3.3.bn2.bias      |    256     |\n","|     resNet.layer3.4.conv1.weight    |   589824   |\n","|      resNet.layer3.4.bn1.weight     |    256     |\n","|       resNet.layer3.4.bn1.bias      |    256     |\n","|     resNet.layer3.4.conv2.weight    |   589824   |\n","|      resNet.layer3.4.bn2.weight     |    256     |\n","|       resNet.layer3.4.bn2.bias      |    256     |\n","|     resNet.layer3.5.conv1.weight    |   589824   |\n","|      resNet.layer3.5.bn1.weight     |    256     |\n","|       resNet.layer3.5.bn1.bias      |    256     |\n","|     resNet.layer3.5.conv2.weight    |   589824   |\n","|      resNet.layer3.5.bn2.weight     |    256     |\n","|       resNet.layer3.5.bn2.bias      |    256     |\n","|     resNet.layer4.0.conv1.weight    |  1179648   |\n","|      resNet.layer4.0.bn1.weight     |    512     |\n","|       resNet.layer4.0.bn1.bias      |    512     |\n","|     resNet.layer4.0.conv2.weight    |  2359296   |\n","|      resNet.layer4.0.bn2.weight     |    512     |\n","|       resNet.layer4.0.bn2.bias      |    512     |\n","| resNet.layer4.0.downsample.0.weight |   131072   |\n","| resNet.layer4.0.downsample.1.weight |    512     |\n","|  resNet.layer4.0.downsample.1.bias  |    512     |\n","|     resNet.layer4.1.conv1.weight    |  2359296   |\n","|      resNet.layer4.1.bn1.weight     |    512     |\n","|       resNet.layer4.1.bn1.bias      |    512     |\n","|     resNet.layer4.1.conv2.weight    |  2359296   |\n","|      resNet.layer4.1.bn2.weight     |    512     |\n","|       resNet.layer4.1.bn2.bias      |    512     |\n","|     resNet.layer4.2.conv1.weight    |  2359296   |\n","|      resNet.layer4.2.bn1.weight     |    512     |\n","|       resNet.layer4.2.bn1.bias      |    512     |\n","|     resNet.layer4.2.conv2.weight    |  2359296   |\n","|      resNet.layer4.2.bn2.weight     |    512     |\n","|       resNet.layer4.2.bn2.bias      |    512     |\n","|           resNet.fc.weight          |   512000   |\n","|            resNet.fc.bias           |    1000    |\n","|    lsta_cell.c_classifier.weight    |   512000   |\n","|     lsta_cell.coupling_fc.weight    |   512000   |\n","|      lsta_cell.conv_i_s.weight      |     9      |\n","|       lsta_cell.conv_i_s.bias       |     1      |\n","|     lsta_cell.conv_i_cam.weight     |     9      |\n","|      lsta_cell.conv_f_s.weight      |     9      |\n","|       lsta_cell.conv_f_s.bias       |     1      |\n","|     lsta_cell.conv_f_cam.weight     |     9      |\n","|      lsta_cell.conv_a_s.weight      |     9      |\n","|       lsta_cell.conv_a_s.bias       |     1      |\n","|     lsta_cell.conv_a_cam.weight     |     9      |\n","|      lsta_cell.conv_o_s.weight      |     9      |\n","|       lsta_cell.conv_o_s.bias       |     1      |\n","|     lsta_cell.conv_o_cam.weight     |     9      |\n","|      lsta_cell.conv_i_x.weight      |  2359296   |\n","|       lsta_cell.conv_i_x.bias       |    512     |\n","|      lsta_cell.conv_i_c.weight      |  2359296   |\n","|      lsta_cell.conv_f_x.weight      |  2359296   |\n","|       lsta_cell.conv_f_x.bias       |    512     |\n","|      lsta_cell.conv_f_c.weight      |  2359296   |\n","|      lsta_cell.conv_c_x.weight      |  2359296   |\n","|       lsta_cell.conv_c_x.bias       |    512     |\n","|      lsta_cell.conv_c_c.weight      |  2359296   |\n","|      lsta_cell.conv_o_x.weight      |  2359296   |\n","|       lsta_cell.conv_o_x.bias       |    512     |\n","|      lsta_cell.conv_o_c.weight      |  2359296   |\n","|              fc.weight              |   31232    |\n","|               fc.bias               |     61     |\n","+-------------------------------------+------------+\n","Total Trainable Params: 41729457\n","40736613\n","41729457\n"],"name":"stdout"},{"output_type":"stream","text":["MyConvLSTACell.ipynb:105: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","MyConvLSTACell.ipynb:106: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvLSTACell.ipynb:108: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvLSTACell.ipynb:109: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","MyConvLSTACell.ipynb:110: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvLSTACell.ipynb:112: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","MyConvLSTACell.ipynb:113: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","MyConvLSTACell.ipynb:114: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DFsP8c_yJ2CU","executionInfo":{"status":"ok","timestamp":1609275601521,"user_tz":-60,"elapsed":982,"user":{"displayName":"Luca Neromoro","photoUrl":"","userId":"08197997045770955362"}},"outputId":"65c642cc-e15c-4f19-f54b-d70c8d37ecc1"},"source":["for i in np.linspace(1, 60, 7, endpoint=False):\r\n","  print(np.floor(i))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1.0\n","9.0\n","17.0\n","26.0\n","34.0\n","43.0\n","51.0\n"],"name":"stdout"}]}]}