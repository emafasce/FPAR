{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MyConvLSTMCell.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"q5Y5Kp1MyBPU"},"source":["import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","\n","class MyConvLSTMCell(nn.Module):\n","\n","    def __init__(self, input_size, hidden_size, kernel_size=3, stride=1, padding=1):\n","        super(MyConvLSTMCell, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.kernel_size = kernel_size\n","        self.stride = stride\n","        self.padding = padding\n","        self.conv_i_xx = nn.Conv2d(input_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding)\n","        self.conv_i_hh = nn.Conv2d(hidden_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding,\n","                                   bias=False)\n","\n","        self.conv_f_xx = nn.Conv2d(input_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding)\n","        self.conv_f_hh = nn.Conv2d(hidden_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding,\n","                                   bias=False)\n","\n","        self.conv_c_xx = nn.Conv2d(input_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding)\n","        self.conv_c_hh = nn.Conv2d(hidden_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding,\n","                                   bias=False)\n","\n","        self.conv_o_xx = nn.Conv2d(input_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding)\n","        self.conv_o_hh = nn.Conv2d(hidden_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding,\n","                                   bias=False)\n","\n","        torch.nn.init.xavier_normal(self.conv_i_xx.weight)\n","        torch.nn.init.constant(self.conv_i_xx.bias, 0)\n","        torch.nn.init.xavier_normal(self.conv_i_hh.weight)\n","\n","        torch.nn.init.xavier_normal(self.conv_f_xx.weight)\n","        torch.nn.init.constant(self.conv_f_xx.bias, 0)\n","        torch.nn.init.xavier_normal(self.conv_f_hh.weight)\n","\n","        torch.nn.init.xavier_normal(self.conv_c_xx.weight)\n","        torch.nn.init.constant(self.conv_c_xx.bias, 0)\n","        torch.nn.init.xavier_normal(self.conv_c_hh.weight)\n","\n","        torch.nn.init.xavier_normal(self.conv_o_xx.weight)\n","        torch.nn.init.constant(self.conv_o_xx.bias, 0)\n","        torch.nn.init.xavier_normal(self.conv_o_hh.weight)\n","\n","    def forward(self, x, state):\n","        if state is None:\n","            state = (Variable(torch.randn(x.size(0), x.size(1), x.size(2), x.size(3)).cuda()),\n","                     Variable(torch.randn(x.size(0), x.size(1), x.size(2), x.size(3)).cuda()))\n","        ht_1, ct_1 = state\n","        it = F.sigmoid(self.conv_i_xx(x) + self.conv_i_hh(ht_1))\n","        ft = F.sigmoid(self.conv_f_xx(x) + self.conv_f_hh(ht_1))\n","        ct_tilde = F.tanh(self.conv_c_xx(x) + self.conv_c_hh(ht_1))\n","        ct = (ct_tilde * it) + (ct_1 * ft)\n","        ot = F.sigmoid(self.conv_o_xx(x) + self.conv_o_hh(ht_1))\n","        ht = ot * F.tanh(ct)\n","        return ht, ct"],"execution_count":null,"outputs":[]}]}